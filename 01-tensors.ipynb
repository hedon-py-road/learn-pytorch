{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= x_data ===========\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "========= np_array ===========\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "========= x_np ===========\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "# 数组、nparray, tensor 之间的互相转换\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data=data)\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(\"========= x_data ===========\")\n",
    "print(x_data)\n",
    "print(\"========= np_array ===========\")\n",
    "print(np_array)\n",
    "print(\"========= x_np ===========\")\n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= x_ones ===========\n",
      "tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "========= x_rand ===========\n",
      "tensor([[0.8876, 0.9421],\n",
      "        [0.4022, 0.7740]])\n"
     ]
    }
   ],
   "source": [
    "# 继承之前的 shape 和数据类型\n",
    "x_ones = torch.ones_like(x_data)\n",
    "print(\"========= x_ones ===========\")\n",
    "print(x_ones)\n",
    "\n",
    "# 继承之前的 shape，并显式指定新的数据类型\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float)\n",
    "print(\"========= x_rand ===========\")\n",
    "print(x_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= rand_tensor ===========\n",
      "tensor([[0.8823, 0.3751, 0.7179],\n",
      "        [0.0544, 0.1165, 0.3968]])\n",
      "========= ones_tensor ===========\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "========= zeros_tensor ===========\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 根据 shape 生成 tensor\n",
    "shape = (2, 3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "print(\"========= rand_tensor ===========\")\n",
    "print(rand_tensor)\n",
    "print(\"========= ones_tensor ===========\")\n",
    "print(ones_tensor)\n",
    "print(\"========= zeros_tensor ===========\")\n",
    "print(zeros_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of tensor: torch.Size([2, 3])\n",
      "datatype of tensor: torch.float32\n",
      "device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "# 获取 tensor 的各种属性\n",
    "print(f\"shape of tensor: {ones_tensor.shape}\")\n",
    "print(f\"datatype of tensor: {ones_tensor.dtype}\")\n",
    "print(f\"device tensor is stored on: {ones_tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moving ones_tensor to mps (Mac GPU)\n",
      "device tensor is stored on: mps:0\n"
     ]
    }
   ],
   "source": [
    "# 对 tensor 进行操作\n",
    "\n",
    "# 转移到 GPU/MPS\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"moving ones_tensor to mps (Mac GPU)\")\n",
    "    ones_tensor = ones_tensor.to('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    print(\"moveing ones_tensor to cuda\")\n",
    "    ones_tensor = ones_tensor.to('cuda')\n",
    "\n",
    "print(f\"device tensor is stored on: {ones_tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first row: tensor([1., 1., 1., 1.])\n",
      "first column: tensor([1., 1., 1., 1.])\n",
      "last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# tensor 索引和切片\n",
    "tensor = torch.ones(4,4)\n",
    "print(f\"first row: {tensor[0]}\") # 第一行\n",
    "print(f\"first column: {tensor[:, 0]}\") # 第一列\n",
    "print(f\"last column: {tensor[..., -1]}\") # 最后一列\n",
    "tensor[:,1] = 0 # 第一列置为0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) torch.Size([12, 4])\n",
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]]) torch.Size([4, 12])\n",
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]]) torch.Size([4, 12])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) torch.Size([12, 4])\n"
     ]
    }
   ],
   "source": [
    "# tensor 合并, dim 按第 i 维拼接，只在第 i 维变长\n",
    "t0 = torch.cat(tensors=[tensor, tensor, tensor], dim=0)\n",
    "print(t0, t0.shape)\n",
    "t1 = torch.cat(tensors=[tensor, tensor, tensor], dim=1) \n",
    "print(t1, t1.shape)\n",
    "t2 = torch.cat(tensors=[tensor, tensor, tensor], dim=-1) \n",
    "print(t2, t2.shape)\n",
    "t3 = torch.cat(tensors=[tensor, tensor, tensor], dim=-2) \n",
    "print(t3, t3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# tensor.stack 堆叠, dim 在第几维扩展一维并拼接\n",
    "# t0 = torch.stack(tensors=[tensor, tensor, tensor], dim=0)\n",
    "# print(t0)\n",
    "# t1 = torch.stack(tensors=[tensor, tensor, tensor], dim=1)\n",
    "# print(t1)\n",
    "# t2 = torch.stack(tensors=[tensor, tensor, tensor], dim=-1)\n",
    "# print(t2)\n",
    "# t3 = torch.stack(tensors=[tensor, tensor, tensor], dim=-2)\n",
    "# print(t3)\n",
    "print(tensor)\n",
    "print(tensor.shape)\n",
    "t4 = torch.stack(tensors=[tensor, tensor, tensor], dim=1)\n",
    "print(t4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor: tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor.T: tensor([[1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "========= y1 ==========\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "========= y2 ==========\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "========= y3 ==========\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "========= z1 ==========\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "========= z2 ==========\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "========= z3 ==========\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor(12.) <class 'torch.Tensor'>\n",
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "# 数学运算\n",
    "print(f\"tensor: {tensor}\")\n",
    "\n",
    "# tensor.T 对二维张量（矩阵）做转置（行列互换），shape 变成 (n, m) → (m, n)。\n",
    "print(f\"tensor.T: {tensor.T}\")\n",
    "\n",
    "# @ 做矩阵乘法（不是逐元素乘法），常用于神经网络的全连接层、特征相关性等\n",
    "y1 = tensor @ tensor.T\n",
    "# @ 运算符和 .matmul() 完全等价，底层都调用 BLAS（高效线性代数库）做高性能乘法。\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "y3 = torch.rand_like(tensor)\n",
    "# 和上面一样，但指定结果保存在 y3 里，避免分配新内存，提升性能（尤其大张量时）。\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "print(\"========= y1 ==========\")\n",
    "print(y1)\n",
    "print(\"========= y2 ==========\")\n",
    "print(y2)\n",
    "print(\"========= y3 ==========\")\n",
    "print(y3)\n",
    "\n",
    "# 两个 shape 完全相同的张量，每个元素相乘，结果 shape 不变。\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)\n",
    "print(\"========= z1 ==========\")\n",
    "print(z1)\n",
    "print(\"========= z2 ==========\")\n",
    "print(z2)\n",
    "print(\"========= z3 ==========\")\n",
    "print(z3)\n",
    "\n",
    "# 对所有元素求和，结果是一个 0 维张量（tensor(XX)）。\n",
    "agg = tensor.sum()\n",
    "print(agg, type(agg))\n",
    "# 把单元素（0 维）张量转换为原生 Python 数值（如 float）。\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= tensor ==========\n",
      "tensor([[11., 10., 11., 11.],\n",
      "        [11., 10., 11., 11.],\n",
      "        [11., 10., 11., 11.],\n",
      "        [11., 10., 11., 11.]])\n",
      "tensor([[16., 15., 16., 16.],\n",
      "        [16., 15., 16., 16.],\n",
      "        [16., 15., 16., 16.],\n",
      "        [16., 15., 16., 16.]])\n",
      "tensor([[16., 15., 16., 16.],\n",
      "        [16., 15., 16., 16.],\n",
      "        [16., 15., 16., 16.],\n",
      "        [16., 15., 16., 16.]])\n",
      "tensor([[21., 20., 21., 21.],\n",
      "        [21., 20., 21., 21.],\n",
      "        [21., 20., 21., 21.],\n",
      "        [21., 20., 21., 21.]])\n"
     ]
    }
   ],
   "source": [
    "print(\"========= tensor ==========\")\n",
    "print(tensor)\n",
    "\n",
    "tensor.add_(5) # 原地修改 tensor\n",
    "print(tensor)\n",
    "\n",
    "tensor_add_5 = tensor.add(5) # 返回修改后的\n",
    "print(tensor)\n",
    "print(tensor_add_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n",
      "[6. 6. 6. 6. 6.]\n"
     ]
    }
   ],
   "source": [
    "# tensor 转 numpy\n",
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")\n",
    "\n",
    "# 修改 tensor 会同步影响 numpy\n",
    "t.add_(5)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# numpy 转 tensor\n",
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)\n",
    "\n",
    "np.add(n, 1, out=n) # 修改 n 一样会同步影响 t\n",
    "print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
